{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_ANN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBO9SfoB9ZV8NWqZw0ewa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKJIN1999/MNIST_ANN/blob/main/MNIST_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple ANN for MNIST dataset**\n",
        "### This project will explain how to build up a simple Neural Network model for MNIST dataset\n",
        "\n",
        "\n",
        "### The process will be as \n",
        "1.  Download MNIST dataset\n",
        "2.  Pre-set the Hyper-parameters for the model\n",
        "3.  Manage the MNIST dataset for training and testing\n",
        "4.  Create Neural Network Model for MNIST\n",
        "5.  Initialize Training, Evaluating process\n",
        "6.  Train the Model and Test\n",
        "\n"
      ],
      "metadata": {
        "id": "U1KCEYV3QsNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Download MNIST dataset"
      ],
      "metadata": {
        "id": "5pXP_vz9QsQT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "0cBozjZiPUuc",
        "outputId": "4526a45f-7999-47ce-c135-80057ea8f275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import random\n",
        "\n",
        "mnist = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "\n",
        "print(mnist.data.size())\n",
        "print(mnist.targets.size())\n",
        "\n",
        "num = random.randint(1,50)\n",
        "\n",
        "plt.imshow(mnist.data[num], cmap=\"Greys\", interpolation= \"nearest\") \n",
        "#interpolation= \"nearest\" (Used to display image without trying to interpolate between pixels if the display resolution is not the same as the image resolution)\n",
        "plt.show()\n",
        "print(mnist.targets[num])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Pre-set the Hyper-parameters for the model"
      ],
      "metadata": {
        "id": "LKAjhh3hUKvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "# cuda is possible or if not, use the cpu (Cuda is a paralle computing platform and application programming interface)\n",
        "# if you see cpu as your result, please change your runtime to GPU \n",
        "print(device)\n",
        "\n",
        "# 28*28 =784\n",
        "input_size = 784 \n",
        "# number of hidden layers\n",
        "hidden_size = 500\n",
        "#output label\n",
        "num_classes = 10\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "learning_rate =0.001\n",
        "drop_prob = 0.5\n",
        "weight_decay_lambda = 1e-4\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yeIeUFEULdw",
        "outputId": "638b9d6d-ccdf-4515-c19a-6ad163eeac68"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Manage the MNIST dataset for training and testing"
      ],
      "metadata": {
        "id": "qSWTBX_zWDhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "#Call Each train validation and testing dataset from MNIST\n",
        "train_val_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# #Train : Validation = 8 : 2\n",
        "# amount = len(train_val_dataset)\n",
        "# train_set = int(amount*0.8)\n",
        "# validation_set = int(amount*0.2)\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [50000,10000])\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Call Dataloader for each dataset\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Note that MNIST datasets contain image and also the labels as well. "
      ],
      "metadata": {
        "id": "oo2pmpiBWDoL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Create Neural Network Model for MNIST"
      ],
      "metadata": {
        "id": "c2dZZq3neQyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#Inherit the parent class nn.Module\n",
        "class MNISTNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes, drop_prob):\n",
        "      super().__init__()\n",
        "      self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "      self.layer2 = nn.Linear(hidden_size, num_classes)\n",
        "      self.activation_fn = nn.ReLU()\n",
        "      self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "      out = self.layer1(x)\n",
        "      out = self.activation_fn(out)\n",
        "      out = self.dropout(out)\n",
        "      out = self.layer2(out)\n",
        "      return out\n",
        "\n",
        "# What .to(device) do \n",
        "# On calling forward it splits the input into multiple chunks (one chunk per GPU), \n",
        "# replicates the underlying model to multiple GPUs, runs forward on each of them, \n",
        "# and gathers the outputs.\n",
        "\n",
        "model = MNISTNet(input_size, hidden_size, num_classes, drop_prob).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Weight_decay decreasese the overfitting by keepin the weights small\n",
        "# Another different example for this is ResNet where Residual Learning adds exact x into F(x) so the weight can always at least have 1 after each backpropogation \n",
        "# However, the residual learning is not used for overfitting, but to prevent the weight decreasing to 0 caused by prolonged training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay_lambda)"
      ],
      "metadata": {
        "id": "F6YVH92teQ46"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Initialize Training, Evaluating process"
      ],
      "metadata": {
        "id": "XqRbVAc_sSqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train (train_loader, model, criterion, optimizer):\n",
        "  loss_array = []\n",
        "  # simply to notify your model that your training ( ! dropout or batchnorm act different from each train and test )\n",
        "  model.train()\n",
        "\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss_array.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('loss: {}'.format(sum(loss_array)/len(loss_array)))\n",
        "      loss_array = []"
      ],
      "metadata": {
        "id": "YzvoKYu3qfPQ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(val_loader, model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()\n",
        "  for i , (images, labels) in enumerate(val_loader):\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "  \n",
        "    # reduce memory usage \n",
        "    with torch.no_grad():\n",
        "      outputs = model(images)\n",
        "      # collect the maximum value from outputs and decrease 1 dim\n",
        "      _,predicted = torch.max(outputs.data, 1)\n",
        "      correct += (predicted==labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "  return correct/total\n"
      ],
      "metadata": {
        "id": "vwUD8V_wxp7y"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Train the Model and Test"
      ],
      "metadata": {
        "id": "ADIZG2Nm5DaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(num_epochs):\n",
        "  train(train_loader, model, criterion, optimizer)\n",
        "  acc = evaluation(val_loader,model)\n",
        "  print(\"accuracy : {}\".format(acc))\n",
        "\n",
        "evaluation(test_loader,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ry7Sznc5DhL",
        "outputId": "a1c96b69-d907-4de3-e91f-662c468d3182"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.7610346075892448\n",
            "loss: 0.3621300099790096\n",
            "loss: 0.27759522303938866\n",
            "loss: 0.2651833222806454\n",
            "loss: 0.2215591612458229\n",
            "accuracy : 0.9419\n",
            "loss: 0.20000383123755455\n",
            "loss: 0.18556306943297385\n",
            "loss: 0.1842260693013668\n",
            "loss: 0.1742337579280138\n",
            "loss: 0.17089924827218056\n",
            "accuracy : 0.9607\n",
            "loss: 0.131922008022666\n",
            "loss: 0.13982236426323652\n",
            "loss: 0.13565645191818476\n",
            "loss: 0.1410462599620223\n",
            "loss: 0.13642106588929892\n",
            "accuracy : 0.9668\n",
            "loss: 0.11703393839299679\n",
            "loss: 0.10889264103025198\n",
            "loss: 0.11727977011352778\n",
            "loss: 0.11050343006849289\n",
            "loss: 0.11293064003810287\n",
            "accuracy : 0.9718\n",
            "loss: 0.10634728483855724\n",
            "loss: 0.0926819565333426\n",
            "loss: 0.10144630175083875\n",
            "loss: 0.10271316774189472\n",
            "loss: 0.09530718002468347\n",
            "accuracy : 0.9731\n",
            "loss: 0.08396233087405562\n",
            "loss: 0.08739632673561573\n",
            "loss: 0.09180301086977125\n",
            "loss: 0.08895773969590665\n",
            "loss: 0.09347226742655039\n",
            "accuracy : 0.9748\n",
            "loss: 0.07820609508082271\n",
            "loss: 0.07688297932036221\n",
            "loss: 0.0829094342328608\n",
            "loss: 0.07911122439429163\n",
            "loss: 0.08128946640528739\n",
            "accuracy : 0.9747\n",
            "loss: 0.07377084261737764\n",
            "loss: 0.07098867570981383\n",
            "loss: 0.07790428252890706\n",
            "loss: 0.0719863441772759\n",
            "loss: 0.07627203854732216\n",
            "accuracy : 0.9768\n",
            "loss: 0.06323485696688294\n",
            "loss: 0.06630888533778488\n",
            "loss: 0.07485509730875492\n",
            "loss: 0.0716728870384395\n",
            "loss: 0.07354549770243465\n",
            "accuracy : 0.9757\n",
            "loss: 0.06331728076562286\n",
            "loss: 0.06403094828128815\n",
            "loss: 0.06636409373953939\n",
            "loss: 0.06901308915577829\n",
            "loss: 0.06718341116793454\n",
            "accuracy : 0.978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9794"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}